{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# üì¶ Core Libraries\n",
    "# =====================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import graphviz\n",
    "\n",
    "from itertools import product\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# =====================================================================\n",
    "# üìä Data Preprocessing & Feature Selection\n",
    "# =====================================================================\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# =====================================================================\n",
    "# üìê Statistics & Diagnostics\n",
    "# =====================================================================\n",
    "from scipy.stats import (\n",
    "    randint, kstest, norm, mannwhitneyu, ttest_ind, chi2_contingency\n",
    ")\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "# =====================================================================\n",
    "# üîç Model Selection & Evaluation\n",
    "# =====================================================================\n",
    "from sklearn.model_selection import (\n",
    "    StratifiedKFold, GridSearchCV, RandomizedSearchCV,\n",
    "    cross_val_predict, cross_val_score\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    confusion_matrix, mean_squared_error, mean_absolute_error,\n",
    "    cohen_kappa_score, matthews_corrcoef, roc_curve,\n",
    "    precision_recall_curve, auc, make_scorer\n",
    ")\n",
    "\n",
    "# =====================================================================\n",
    "# ü§ñ Machine Learning Models\n",
    "# =====================================================================\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_graphviz\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# =====================================================================\n",
    "# üöÄ Gradient Boosting Libraries\n",
    "# =====================================================================\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# =====================================================================\n",
    "# ‚öñÔ∏è Imbalanced Data Handling\n",
    "# =====================================================================\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# =====================================================================\n",
    "# üíæ Persistence & Metadata\n",
    "# =====================================================================\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# =====================================================================\n",
    "# üìç Utilities\n",
    "# =====================================================================\n",
    "from kneed import KneeLocator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "ROOT = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "DATA_DIR    = ROOT / \"data\"\n",
    "MODELS_DIR  = ROOT / \"models\"\n",
    "OUTPUTS_DIR = ROOT / \"outputs\"\n",
    "for d in (DATA_DIR, MODELS_DIR, OUTPUTS_DIR): d.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"ROOT:\", ROOT)  # sanity check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# üìÇ Data Loading\n",
    "# =====================================================================\n",
    "\n",
    "ROOT = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "DATA_DIR    = ROOT / \"data\"\n",
    "MODELS_DIR  = ROOT / \"models\"\n",
    "OUTPUTS_DIR = ROOT / \"outputs\"\n",
    "for d in (DATA_DIR, MODELS_DIR, OUTPUTS_DIR):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TRAIN_FILE = DATA_DIR / \"lungCancer.csv\"   # default\n",
    "TEST_FILE  = DATA_DIR / \"test_set.csv\"     # default\n",
    "\n",
    "def load_any(path, **kw):\n",
    "    path = Path(path)\n",
    "    if path.suffix.lower() in (\".xlsx\", \".xls\"):\n",
    "        return pd.read_excel(path, **kw)\n",
    "    return pd.read_csv(path, **kw)\n",
    "\n",
    "try:\n",
    "    df       = load_any(TRAIN_FILE, skiprows=1)\n",
    "    test_set = load_any(TEST_FILE,  encoding=\"ISO-8859-1\")\n",
    "except FileNotFoundError:\n",
    "    # fallback picker (local GUI)\n",
    "    try:\n",
    "        import tkinter as tk\n",
    "        from tkinter import filedialog\n",
    "\n",
    "        root = tk.Tk()\n",
    "        root.withdraw()\n",
    "        root.attributes(\"-topmost\", True)  # bring dialogs to front\n",
    "\n",
    "        tr = Path(filedialog.askopenfilename(\n",
    "            title=\"Select TRAINING file (.csv/.xlsx)\",\n",
    "            filetypes=[(\"Supported\", \"*.csv *.xlsx *.xls\"), (\"All files\", \"*.*\")],\n",
    "            parent=root\n",
    "        ))\n",
    "        te = Path(filedialog.askopenfilename(\n",
    "            title=\"Select TEST file (.csv/.xlsx)\",\n",
    "            filetypes=[(\"Supported\", \"*.csv *.xlsx *.xls\"), (\"All files\", \"*.*\")],\n",
    "            parent=root\n",
    "        ))\n",
    "        root.destroy()\n",
    "\n",
    "        if not tr or not te:\n",
    "            raise FileNotFoundError(\"You must select both files.\")\n",
    "\n",
    "        df       = load_any(tr, skiprows=1)\n",
    "        test_set = load_any(te, encoding=\"ISO-8859-1\")\n",
    "\n",
    "    except Exception as e:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Couldn‚Äôt find {TRAIN_FILE.name} or {TEST_FILE.name} in {DATA_DIR}. \"\n",
    "            \"Either place your files there or use the file picker.\"\n",
    "        ) from e\n",
    "\n",
    "print(\"Training shape:\", df.shape, \"| Test shape:\", test_set.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# üßπ Data Cleaning & Preprocessing\n",
    "# =====================================================================\n",
    "\n",
    "# --- Drop unnecessary columns from Test Set ---\n",
    "test_set.drop(columns=[\"ID\", \"Diagnosis\", \"'Cancer stage'\"], inplace=True)\n",
    "\n",
    "# --- Clean and standardize column names ---\n",
    "test_set.columns = [\n",
    "    col.replace(\"_\", \" \").strip().replace('\"', \"\") for col in test_set.columns\n",
    "]\n",
    "\n",
    "# --- Rename specific columns for clarity ---\n",
    "test_set.rename(\n",
    "    columns={\n",
    "        \"Sex binarized\": \"Sex\",\n",
    "        \"Smoking status binarized\": \"Smoking status\",\n",
    "        \"ALB GLB ratio\": \"ALB/GLB\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "# --- Normalize labels in Test Set (-1 ‚Üí 0, 1 ‚Üí 1) ---\n",
    "test_set[\"Label\"] = test_set[\"Label\"].replace({-1: 0, 1: 1})\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# üîÑ Training & Validation Set Preparation\n",
    "# =====================================================================\n",
    "\n",
    "# --- Drop unnecessary columns ---\n",
    "df.drop(columns=[\"ID\", \"Diagnosis\", \"Cancer stage\", \"Predicted\"], inplace=True)\n",
    "\n",
    "# --- Split into Training and Validation sets (based on 'Dataset' column) ---\n",
    "training_set = df[df[\"Dataset\"] == \"Trainging set\"].copy()\n",
    "validation_set = df[df[\"Dataset\"] == \"Test set\"].copy()\n",
    "\n",
    "# --- Drop 'Dataset' column if no longer needed ---\n",
    "training_set.drop(columns=[\"Dataset\"], inplace=True)\n",
    "validation_set.drop(columns=[\"Dataset\"], inplace=True)\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# üë§ Encode Categorical Variables\n",
    "# =====================================================================\n",
    "\n",
    "# --- Convert 'Sex' to binary (male=1, female=0) ---\n",
    "training_set[\"Sex\"] = training_set[\"Sex\"].map({\"male\": 1, \"female\": 0})\n",
    "validation_set[\"Sex\"] = validation_set[\"Sex\"].map({\"male\": 1, \"female\": 0})\n",
    "\n",
    "# --- Convert 'Smoking status' to binary (Yes=1, No=0) ---\n",
    "training_set[\"Smoking status\"] = training_set[\"Smoking status\"].map({\"Yes\": 1, \"No\": 0})\n",
    "validation_set[\"Smoking status\"] = validation_set[\"Smoking status\"].map({\"Yes\": 1, \"No\": 0})\n",
    "\n",
    "# --- Convert 'Label' to binary (1=True, 0=False) ---\n",
    "print(\"Unique labels before conversion:\", training_set[\"Label\"].unique())\n",
    "training_set[\"Label\"] = (training_set[\"Label\"] == 1).astype(int)\n",
    "validation_set[\"Label\"] = (validation_set[\"Label\"] == 1).astype(int)\n",
    "print(\"Label distribution after conversion:\\n\", training_set[\"Label\"].value_counts(dropna=False))\n",
    "\n",
    "# =====================================================================\n",
    "# üîç Missing Values Check\n",
    "# =====================================================================\n",
    "\n",
    "# --- Count missing values per column in Training Set ---\n",
    "missing_counts = training_set.isnull().sum()\n",
    "\n",
    "# --- Keep only columns with missing values ---\n",
    "missing_columns = missing_counts[missing_counts > 0]\n",
    "\n",
    "# --- Display missing values summary ---\n",
    "print(\"Missing values in training set:\\n\", missing_columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# üìä Histograms (chunked 4√ó4 grids)\n",
    "# =====================================================================\n",
    "\n",
    "def chunk_columns(columns, chunk_size=16):\n",
    "    \"\"\"Split a list of columns into chunks of size `chunk_size`.\"\"\"\n",
    "    return [columns[i:i + chunk_size] for i in range(0, len(columns), chunk_size)]\n",
    "\n",
    "all_cols = training_set.columns.tolist()\n",
    "groups = chunk_columns(all_cols)\n",
    "\n",
    "def plot_mixed_grid(cols):\n",
    "    \"\"\"Plot numeric cols as histograms; non-numeric as bar charts.\"\"\"\n",
    "    rows = math.ceil(len(cols) / 4)\n",
    "    fig, axes = plt.subplots(rows, 4, figsize=(18, rows * 4.2))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, col in enumerate(cols):\n",
    "        ax = axes[i]\n",
    "        data = training_set[col].dropna()\n",
    "\n",
    "        if pd.api.types.is_numeric_dtype(data):\n",
    "            sns.histplot(data, bins=30, ax=ax, color='teal')\n",
    "        else:\n",
    "            counts = data.value_counts()\n",
    "            sns.barplot(x=counts.index, y=counts.values, ax=ax, palette='Set2')\n",
    "            ax.set_xticklabels(counts.index, rotation=90, fontsize=8)\n",
    "\n",
    "        ax.set_title(col, fontsize=10)\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_ylabel('')\n",
    "        ax.tick_params(axis='x', labelbottom=True)\n",
    "        ax.tick_params(axis='y', labelleft=True)\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for j in range(len(cols), len(axes)):\n",
    "        axes[j].set_visible(False)\n",
    "\n",
    "    plt.tight_layout(pad=3.5)\n",
    "    plt.subplots_adjust(top=0.94, hspace=0.4)\n",
    "    plt.show()\n",
    "\n",
    "for group in groups:\n",
    "    plot_mixed_grid(group)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# üß™ Normality Testing (Kolmogorov‚ÄìSmirnov)\n",
    "# =====================================================================\n",
    "\n",
    "exclude_cols = ['Label', 'Sex', 'Smoking status']\n",
    "numeric_cols = training_set.select_dtypes(include='number').columns.drop(exclude_cols)\n",
    "\n",
    "normality_results = []\n",
    "for col in numeric_cols:\n",
    "    data = training_set[col].dropna()\n",
    "    standardized = (data - data.mean()) / data.std()\n",
    "    stat, p_value = kstest(standardized, 'norm')\n",
    "\n",
    "    normality_results.append({\n",
    "        'Attribute': col,\n",
    "        'K‚ÄìS Statistic': round(stat, 4),\n",
    "        'p-value': '<0.001' if p_value < 0.001 else round(p_value, 4),\n",
    "        'Normality': 'Not normal' if p_value < 0.001 else 'Normal'\n",
    "    })\n",
    "\n",
    "normality_df = pd.DataFrame(normality_results)\n",
    "print(normality_df.to_string(index=False))\n",
    "\n",
    "# Export normality results\n",
    "normality_df.to_csv(OUTPUTS_DIR / \"normality_results.csv\", index=False)\n",
    "print(\"üíæ Saved: outputs/normality_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# üìã Table 1: Group Comparisons (Categorical: œá¬≤; Numeric: t-test / Mann‚ÄìWhitney)\n",
    "# =====================================================================\n",
    "\n",
    "# Split by class\n",
    "lc = training_set[training_set['Label'] == 1]\n",
    "non_lc = training_set[training_set['Label'] == 0]\n",
    "\n",
    "# Pre-labeled non-normal features (from K‚ÄìS)\n",
    "non_normal_features = {\n",
    "    'White blood cell', 'Eosinophil ratio', 'Basophil ratio', 'Neutrophilic granulocytes',\n",
    "    'Eosinophil cells', 'Basophil count', 'Red blood cell distribution width - CV',\n",
    "    'Platelet distribution width', 'Glucose', 'Alanine transaminase', 'Aspartate aminotransferase',\n",
    "    'AST/ALT', 'Glutamyl Transferase', 'Alkaline phosphatase ', 'Creatine kinase',\n",
    "    'Creatine kinase isoenzymes', 'Lactate dehydrogenase', 'Amylase'\n",
    "}\n",
    "\n",
    "summary = []\n",
    "\n",
    "# --- Categorical variables (œá¬≤) ---\n",
    "cat_labels = {'Sex': 'Sex (m/f)', 'Smoking status': 'Smoking status (y/n)'}\n",
    "for col, label in cat_labels.items():\n",
    "    lc_counts = lc[col].value_counts().to_dict()\n",
    "    non_counts = non_lc[col].value_counts().to_dict()\n",
    "\n",
    "    lc_str = f\"{lc_counts.get(1,0)}/{lc_counts.get(0,0)}\"\n",
    "    non_str = f\"{non_counts.get(1,0)}/{non_counts.get(0,0)}\"\n",
    "\n",
    "    contingency = [[lc_counts.get(1,0), lc_counts.get(0,0)],\n",
    "                   [non_counts.get(1,0), non_counts.get(0,0)]]\n",
    "    _, p, _, _ = chi2_contingency(contingency)\n",
    "    p_val = \"<0.001\" if p < 0.001 else round(p, 3)\n",
    "\n",
    "    summary.append([label, \"\", lc_str, non_str, p_val])\n",
    "\n",
    "# --- Numeric variables (t-test or Mann‚ÄìWhitney) ---\n",
    "num_cols = ['Age'] + training_set.select_dtypes(include='number') \\\n",
    "    .columns.drop(['Label', 'Sex', 'Smoking status', 'Age']).tolist()\n",
    "\n",
    "for col in num_cols:\n",
    "    lc_vals = lc[col].dropna()\n",
    "    non_vals = non_lc[col].dropna()\n",
    "\n",
    "    ref_min = training_set[col].min()\n",
    "    ref_max = training_set[col].max()\n",
    "    ref_interval = f\"{ref_min:.2f}-{ref_max:.2f}\"\n",
    "\n",
    "    lc_median = lc_vals.median()\n",
    "    lc_iqr = lc_vals.quantile(0.75) - lc_vals.quantile(0.25)\n",
    "    non_median = non_vals.median()\n",
    "    non_iqr = non_vals.quantile(0.75) - non_vals.quantile(0.25)\n",
    "\n",
    "    lc_str = f\"{lc_median:.2f} ({lc_iqr:.2f})\"\n",
    "    non_str = f\"{non_median:.2f} ({non_iqr:.2f})\"\n",
    "\n",
    "    if col in non_normal_features:\n",
    "        stat, p = mannwhitneyu(lc_vals, non_vals, alternative='two-sided')\n",
    "        display_col = col + \"*\"  # flag non-normal\n",
    "    else:\n",
    "        stat, p = ttest_ind(lc_vals, non_vals, equal_var=False)\n",
    "        display_col = col\n",
    "\n",
    "    p_val = \"<0.001\" if p < 0.001 else round(p, 3)\n",
    "    summary.append([display_col, ref_interval, lc_str, non_str, p_val])\n",
    "\n",
    "df_summary = pd.DataFrame(\n",
    "    summary,\n",
    "    columns=[\"Attribute\", \"Reference interval\", \"LC (n=183)\", \"Non-LC (n=94)\", \"p-value\"]\n",
    ")\n",
    "\n",
    "# Put Age first with a nicer label\n",
    "df_summary.loc[df_summary['Attribute'] == 'Age', 'Attribute'] = 'Age, years'\n",
    "df_summary = pd.concat([\n",
    "    df_summary[df_summary['Attribute'] == 'Age, years'],\n",
    "    df_summary[df_summary['Attribute'] != 'Age, years']\n",
    "]).reset_index(drop=True)\n",
    "\n",
    "print(df_summary.to_string(index=False))\n",
    "\n",
    "# ‚úÖ Export Table 1 to CSV\n",
    "import os\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "df_summary.to_csv(OUTPUTS_DIR/ \"table1_summary.csv\", index=False)\n",
    "print(\"üíæ Saved: outputs/table1_summary.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# üö® Outliers (IQR-based)\n",
    "# =====================================================================\n",
    "\n",
    "# 1) Numeric columns\n",
    "numerical_cols = training_set.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# 2) Compute IQR bounds on training set\n",
    "def compute_iqr_bounds(df, cols):\n",
    "    bounds = {}\n",
    "    for col in cols:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        bounds[col] = (Q1 - 1.5 * IQR, Q3 + 1.5 * IQR)\n",
    "    return bounds\n",
    "\n",
    "iqr_bounds = compute_iqr_bounds(training_set, numerical_cols)\n",
    "\n",
    "# 3) Flag outliers per column\n",
    "def flag_outliers(df, bounds):\n",
    "    flags = pd.DataFrame(False, index=df.index, columns=bounds.keys())\n",
    "    for col, (lower, upper) in bounds.items():\n",
    "        flags[col] = ~df[col].between(lower, upper)\n",
    "    return flags\n",
    "\n",
    "flags_train = flag_outliers(training_set, iqr_bounds)\n",
    "\n",
    "# 4) Count how many features are outliers per row\n",
    "training_set['outlier_count'] = flags_train.sum(axis=1)\n",
    "\n",
    "# 5) Styled Excel with outliers highlighted\n",
    "def highlight_outliers(val, is_outlier):\n",
    "    return 'background-color: orange' if is_outlier else ''\n",
    "\n",
    "styled = training_set[numerical_cols].style.apply(\n",
    "    lambda col: [highlight_outliers(val, outlier) for val, outlier in zip(col, flags_train[col.name])],\n",
    "    axis=0\n",
    ")\n",
    "styled.to_excel('../outputs/highlighted_outliers.xlsx', engine='openpyxl')\n",
    "\n",
    "# Raw flags snapshot\n",
    "flags_train.to_csv('../outputs/outlier_flags_train.csv', index=False)\n",
    "\n",
    "# Quick boxplot viz\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x=training_set['Label'], y=training_set['outlier_count'], palette='Set3')\n",
    "plt.title(\"Outlier Count Distribution by Label\", fontsize=14)\n",
    "plt.xlabel(\"Label\", fontsize=12)\n",
    "plt.ylabel(\"Outlier Count\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Clean up helper column\n",
    "training_set.drop(columns=['outlier_count'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# üî• Correlation Analysis (blocks + pairs > |0.8|)\n",
    "# =====================================================================\n",
    "\n",
    "# Select numeric attributes (exclude label-like columns)\n",
    "exclude_cols = ['Label', 'diagnosis', 'class', 'target']\n",
    "numerical_cols = [\n",
    "    col for col in training_set.select_dtypes(include=['float64', 'int64']).columns\n",
    "    if col not in exclude_cols\n",
    "]\n",
    "\n",
    "def lower_triangle_corr_heatmap_blocks(df, cols, chunk_size=18, threshold=0.8):\n",
    "    corr_full = df[cols].corr().round(2)\n",
    "    chunks = [cols[i:i + chunk_size] for i in range(0, len(cols), chunk_size)]\n",
    "    block_pairs = list(product(range(len(chunks)), repeat=2))  # (row_chunk, col_chunk)\n",
    "\n",
    "    for r_idx, c_idx in block_pairs:\n",
    "        rows = chunks[r_idx]\n",
    "        cols_ = chunks[c_idx]\n",
    "        sub_corr = corr_full.loc[rows, cols_]\n",
    "\n",
    "        # Masking logic for lower triangle\n",
    "        if r_idx == c_idx:\n",
    "            mask = np.triu(np.ones(sub_corr.shape), k=0).astype(bool)\n",
    "        else:\n",
    "            mask = np.ones(sub_corr.shape, dtype=bool)\n",
    "            for i in range(sub_corr.shape[0]):\n",
    "                for j in range(sub_corr.shape[1]):\n",
    "                    if i > j:\n",
    "                        mask[i, j] = False\n",
    "\n",
    "        # Annotate strong off-diagonal correlations only\n",
    "        annotations = np.full(sub_corr.shape, '', dtype=object)\n",
    "        for i in range(sub_corr.shape[0]):\n",
    "            for j in range(sub_corr.shape[1]):\n",
    "                is_not_masked = not mask[i, j]\n",
    "                is_off_diagonal = rows[i] != cols_[j]\n",
    "                is_strong = abs(sub_corr.iat[i, j]) > threshold\n",
    "                if is_not_masked and is_off_diagonal and is_strong:\n",
    "                    annotations[i, j] = f\"{sub_corr.iat[i, j]:.2f}\"\n",
    "\n",
    "        # Bold borders for strong off-diagonal cells\n",
    "        border_mask = (\n",
    "            (np.abs(sub_corr) > threshold) &\n",
    "            (~mask) &\n",
    "            pd.DataFrame([[rows[i] != cols_[j] for j in range(len(cols_))]\n",
    "                          for i in range(len(rows))],\n",
    "                         index=sub_corr.index, columns=sub_corr.columns)\n",
    "        )\n",
    "\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        ax = sns.heatmap(\n",
    "            sub_corr, mask=mask, annot=annotations, fmt='',\n",
    "            cmap='coolwarm', vmin=-1, vmax=1,\n",
    "            linewidths=0.5, linecolor='white',\n",
    "            cbar_kws={\"shrink\": 0.75}\n",
    "        )\n",
    "\n",
    "        for y in range(sub_corr.shape[0]):\n",
    "            for x in range(sub_corr.shape[1]):\n",
    "                if border_mask.iat[y, x]:\n",
    "                    ax.add_patch(plt.Rectangle((x, y), 1, 1, fill=False, edgecolor='black', lw=2))\n",
    "\n",
    "        plt.title(f'üìä Correlation Block ‚Äî Rows {r_idx+1}, Cols {c_idx+1} (Only |r| > {threshold})', fontsize=13)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "lower_triangle_corr_heatmap_blocks(training_set, numerical_cols, chunk_size=18, threshold=0.8)\n",
    "\n",
    "# Upper-triangle pairs |r| > 0.8 (no duplicates)\n",
    "corr_matrix = training_set[numerical_cols].corr().round(2)\n",
    "mask = np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    "\n",
    "high_corr_pairs = (\n",
    "    corr_matrix.where(mask)\n",
    "    .stack()\n",
    "    .reset_index()\n",
    "    .rename(columns={0: 'Correlation', 'level_0': 'Attribute 1', 'level_1': 'Attribute 2'})\n",
    ")\n",
    "\n",
    "high_corr_pairs = high_corr_pairs[\n",
    "    (high_corr_pairs['Correlation'].abs() > 0.8) & (high_corr_pairs['Correlation'].abs() < 1.0)\n",
    "].sort_values(by='Correlation', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Export high-corr pairs\n",
    "high_corr_pairs.to_csv(OUTPUTS_DIR/ \"high_corr_pairs.csv\", index=False)\n",
    "\n",
    "high_corr_pairs  # display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# üßÆ Variance Inflation Factor (VIF) ‚Äî targeted attributes\n",
    "# =====================================================================\n",
    "\n",
    "vif_pairs = [\n",
    "        (\"White blood cell\", \"Neutrophilic granulocytes\"),\n",
    "        (\"Hemoglobin\", \"Hematocrit\"),\n",
    "        (\"Eosinophil ratio\", \"Eosinophil cells\"),\n",
    "        (\"Platelet\", \"Plateletcrit\"),\n",
    "        (\"Red blood cell count\", \"Hematocrit\"),\n",
    "        (\"Total bilirubin\", \"Indirect bilirubin\"),\n",
    "        (\"Total cholesterol\", \"Low-density lipoprotein\"),\n",
    "        (\"Red blood cell count\", \"Hemoglobin\"),\n",
    "        (\"Mean corpuscular volume\", \"Mean corpuscular hemoglobin\"),\n",
    "        (\"Mean platelet volume\", \"Large platelet ratio\"),\n",
    "        (\"Globulin\", \"ALB/GLB\"),\n",
    "        (\"Neutrophile granulocyte ratio\", \"Lymphocyte ratio\"),\n",
    "]\n",
    "\n",
    "vif_attributes = sorted({attr for pair in vif_pairs for attr in pair})\n",
    "# Only keep attributes that exist in training_set\n",
    "attrs = [a for a in vif_attributes if a in training_set.columns]\n",
    "missing = sorted(set(vif_attributes) - set(attrs))\n",
    "if missing:\n",
    "        print(f\"‚ö†Ô∏è Skipping missing attributes not in training_set: {missing}\")\n",
    "\n",
    "vif_df = training_set[attrs].copy().dropna()\n",
    "X = add_constant(vif_df)\n",
    "\n",
    "vif_results = pd.DataFrame({\n",
    "\"Attribute\": X.columns,\n",
    "\"VIF\": [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "})\n",
    "\n",
    "vif_results = (\n",
    "vif_results[vif_results[\"Attribute\"] != \"const\"]\n",
    ".sort_values(by=\"VIF\", ascending=False)\n",
    ")\n",
    "\n",
    "# Map attribute -> VIF from your computed table\n",
    "vif_map = dict(zip(vif_results[\"Attribute\"], vif_results[\"VIF\"]))\n",
    "\n",
    "# Build pair-wise table\n",
    "rows = []\n",
    "for a1, a2 in vif_pairs:\n",
    "    rows.append({\n",
    "        \"Attribute 1\": a1,\n",
    "        \"VIF 1\": vif_map.get(a1, np.nan),\n",
    "        \"Attribute 2\": a2,\n",
    "        \"VIF 2\": vif_map.get(a2, np.nan),\n",
    "        # \"Dropped\": \"\"  # ‚Üê optional: add your decision text here if you want a 'Dropped' column\n",
    "    })\n",
    "\n",
    "vif_pairs_df = pd.DataFrame(rows)\n",
    "\n",
    "# Round & order columns\n",
    "cols = [\"Attribute 1\", \"VIF 1\", \"Attribute 2\", \"VIF 2\"]\n",
    "vif_pairs_df = vif_pairs_df[cols]\n",
    "vif_pairs_df[\"VIF 1\"] = vif_pairs_df[\"VIF 1\"].round(2)\n",
    "vif_pairs_df[\"VIF 2\"] = vif_pairs_df[\"VIF 2\"].round(2)\n",
    "\n",
    "# Save CSV (pair format)\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "vif_pairs_df.to_csv(OUTPUTS_DIR/ \"vif_pairs_results.csv\", index=False)\n",
    "print(\"üíæ Saved: outputs/vif_pairs_results.csv\")\n",
    "\n",
    "# Pretty display (Notebook)\n",
    "styler = (\n",
    "    vif_pairs_df.style\n",
    "    .set_caption(\"High Correlation Pairs with VIF Scores\")\n",
    "    .format({\"VIF 1\": \"{:.2f}\", \"VIF 2\": \"{:.2f}\"})\n",
    ")\n",
    "\n",
    "# Hide index (compat for pandas versions)\n",
    "try:\n",
    "    styler = styler.hide(axis=\"index\")\n",
    "except Exception:\n",
    "    try:\n",
    "        styler = styler.hide_index()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "styler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# üßπ Feature Reduction ‚Äî Drop Multicollinear Attributes\n",
    "# =====================================================================\n",
    "\n",
    "# Dropped features due to high correlation or redundancy:\n",
    "#   Neutrophilic granulocytes, Hematocrit, Eosinophil cells, Plateletcrit,\n",
    "#   Indirect bilirubin, Low-density lipoprotein, Mean corpuscular hemoglobin,\n",
    "#   Large platelet ratio, ALB/GLB, Neutrophile granulocyte ratio\n",
    "#   (also 'Label' separately handled for validation_set)\n",
    "# =====================================================================\n",
    "\n",
    "# --- Training set cleanup ---\n",
    "training_set.drop(\n",
    "    columns=[\n",
    "        'Neutrophilic granulocytes', 'Hematocrit', 'Eosinophil cells',\n",
    "        'Plateletcrit', 'Indirect bilirubin', 'Low-density lipoprotein',\n",
    "        'Mean corpuscular hemoglobin', 'Large platelet ratio',\n",
    "        'ALB/GLB', 'Neutrophile granulocyte ratio'\n",
    "    ],\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "# --- Validation set cleanup ---\n",
    "# Preserve Label separately for evaluation\n",
    "validation_set_label = validation_set['Label'].copy()\n",
    "\n",
    "columns_to_drop = [\n",
    "    'Neutrophilic granulocytes', 'Hematocrit', 'Eosinophil cells',\n",
    "    'Plateletcrit', 'Indirect bilirubin', 'Low-density lipoprotein',\n",
    "    'Mean corpuscular hemoglobin', 'Large platelet ratio',\n",
    "    'ALB/GLB', 'Neutrophile granulocyte ratio', 'Label'\n",
    "]\n",
    "validation_set = validation_set.drop(columns=columns_to_drop)\n",
    "\n",
    "# =====================================================================\n",
    "# ‚öñÔ∏è Class Balance Check (Training Set)\n",
    "# =====================================================================\n",
    "\n",
    "print(\"Class distribution (Training Set):\")\n",
    "print(training_set['Label'].value_counts())\n",
    "\n",
    "sns.countplot(x=training_set['Label'], palette='Set2')\n",
    "plt.title(\"Label Distribution (Training Set)\")\n",
    "plt.xlabel(\"Label (0 = Non-LC, 1 = LC)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# üîÑ K-Fold Cross-Validation with SMOTE\n",
    "# =====================================================================\n",
    "\n",
    "# --- Define candidate models ---\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1_000_000, random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"Support Vector Machine\": SVC(probability=True, random_state=42),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"LightGBM\": LGBMClassifier(random_state=42),\n",
    "    \"AdaBoost\": AdaBoostClassifier(random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "}\n",
    "\n",
    "# --- Features & Target ---\n",
    "X = training_set.drop(columns='Label')  # Features\n",
    "y = training_set['Label']               # Target\n",
    "\n",
    "# --- Store results ---\n",
    "cv_results = []\n",
    "\n",
    "# =====================================================================\n",
    "# üöÄ Run CV for different fold values (k=5,7,10)\n",
    "# =====================================================================\n",
    "for k in [5, 7, 10]:\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    for model_name, base_model in models.items():\n",
    "        # Collect fold metrics\n",
    "        metrics = {'Accuracy': [], 'Sensitivity': [], 'Specificity': [], 'Precision': [], 'AUC': []}\n",
    "\n",
    "        for train_idx, test_idx in skf.split(X, y):\n",
    "            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "            # --- Pipeline: SMOTE + Classifier ---\n",
    "            pipe = ImbPipeline([\n",
    "                ('smote', SMOTE(sampling_strategy=1.0, k_neighbors=5, random_state=42)),\n",
    "                ('clf', base_model)\n",
    "            ])\n",
    "            pipe.fit(X_train, y_train)\n",
    "\n",
    "            # Predictions\n",
    "            y_pred = pipe.predict(X_test)\n",
    "            y_prob = pipe.predict_proba(X_test)[:, 1]\n",
    "\n",
    "            # Confusion matrix breakdown\n",
    "            tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "            # Store metrics\n",
    "            metrics['Accuracy'].append(accuracy_score(y_test, y_pred) * 100)\n",
    "            metrics['Sensitivity'].append(tp / (tp + fn))\n",
    "            metrics['Specificity'].append(tn / (tn + fp))\n",
    "            metrics['Precision'].append(precision_score(y_test, y_pred))\n",
    "            metrics['AUC'].append(roc_auc_score(y_test, y_prob))\n",
    "\n",
    "        # --- Aggregate mean performance ---\n",
    "        cv_results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"CV Folds\": k,\n",
    "            \"Accuracy (%)\": np.mean(metrics['Accuracy']),\n",
    "            \"Sensitivity\": np.mean(metrics['Sensitivity']),\n",
    "            \"Specificity\": np.mean(metrics['Specificity']),\n",
    "            \"Precision\": np.mean(metrics['Precision']),\n",
    "            \"AUC\": np.mean(metrics['AUC'])\n",
    "        })\n",
    "\n",
    "# =====================================================================\n",
    "# üìä Final Results Table\n",
    "# =====================================================================\n",
    "results_df = pd.DataFrame(cv_results)\n",
    "print(results_df.round(3))\n",
    "\n",
    "# --- Export results to CSV ---\n",
    "results_df.to_csv(OUTPUTS_DIR/ \"kfold_cv_results.csv\", index=False)\n",
    "print(\"\\n‚úÖ Cross-validation results saved to 'outputs/kfold_cv_results.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# üéØ GRIDSEARCHCV for Hyperparameter Tuning\n",
    "# =====================================================================\n",
    "\n",
    "# --- Custom Scorers ---\n",
    "def specificity_score(y_true, y_pred):\n",
    "    \"\"\"Calculate specificity = TN / (TN + FP).\"\"\"\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return tn / (tn + fp)\n",
    "\n",
    "def custom_auc(y_true, y_pred_proba):\n",
    "    \"\"\"Custom AUC scorer using predicted probabilities.\"\"\"\n",
    "    return roc_auc_score(y_true, y_pred_proba)\n",
    "\n",
    "# --- Multi-metric scoring dictionary ---\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'sensitivity': make_scorer(recall_score),\n",
    "    'specificity': make_scorer(specificity_score),\n",
    "    'precision': make_scorer(precision_score),\n",
    "    'roc_auc': make_scorer(custom_auc, needs_proba=True)\n",
    "}\n",
    "\n",
    "# =====================================================================\n",
    "# ‚öôÔ∏è Model Configurations + Search Spaces\n",
    "# =====================================================================\n",
    "models_grids = {\n",
    "    \"Logistic Regression\": {\n",
    "        \"model\": LogisticRegression(max_iter=10000, random_state=42),  # try max_iter=-1 for infinity?\n",
    "        \"params\": {\n",
    "            'clf__C': [0.01, 0.1, 1, 10],\n",
    "            'clf__penalty': ['l2'],\n",
    "            'clf__solver': ['liblinear', 'lbfgs'],\n",
    "            'clf__class_weight': [None, 'balanced'],\n",
    "            'clf__fit_intercept': [True, False]\n",
    "        },\n",
    "        \"cv\": 5\n",
    "    },\n",
    "    \"Naive Bayes\": {\n",
    "        \"model\": GaussianNB(),\n",
    "        \"params\": {\n",
    "            'clf__var_smoothing': [1e-9, 1e-8, 1e-7]\n",
    "        },\n",
    "        \"cv\": 5\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        \"model\": RandomForestClassifier(random_state=42),\n",
    "        \"params\": {\n",
    "            'clf__n_estimators': [100, 200],\n",
    "            'clf__max_depth': [5, 10, 15],\n",
    "            'clf__min_samples_split': [2, 5],\n",
    "            'clf__min_samples_leaf': [2, 4],\n",
    "            'clf__max_features': ['sqrt', 'log2'],\n",
    "            'clf__class_weight': ['balanced'],\n",
    "            'clf__criterion': ['gini', 'entropy']\n",
    "        },\n",
    "        \"cv\": 7\n",
    "    },\n",
    "    \"AdaBoost\": {\n",
    "        \"model\": AdaBoostClassifier(random_state=42),\n",
    "        \"params\": {\n",
    "            'clf__n_estimators': [50, 100, 200],\n",
    "            'clf__learning_rate': [0.01, 0.1, 0.5, 1.0],\n",
    "            'clf__algorithm': ['SAMME', 'SAMME.R'],\n",
    "            'clf__estimator': [DecisionTreeClassifier(max_depth=d) for d in [1, 2, 3]]\n",
    "        },\n",
    "        \"cv\": 5\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"model\": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),\n",
    "        \"params\": {\n",
    "            'clf__n_estimators': [100, 200],\n",
    "            'clf__max_depth': [3, 6],\n",
    "            'clf__learning_rate': [0.01, 0.1],\n",
    "            'clf__subsample': [0.7, 1.0],\n",
    "            'clf__colsample_bytree': [0.7, 1.0],\n",
    "            'clf__gamma': [0, 1],\n",
    "            'clf__min_child_weight': [1, 3],\n",
    "            'clf__reg_alpha': [0, 0.1],\n",
    "            'clf__reg_lambda': [1, 5],\n",
    "            'clf__scale_pos_weight': [1, 2]\n",
    "        },\n",
    "        \"cv\": 7\n",
    "    },\n",
    "    \"LightGBM\": {\n",
    "        \"model\": LGBMClassifier(random_state=42),\n",
    "        \"params\": {\n",
    "            'clf__n_estimators': [100, 200],\n",
    "            'clf__max_depth': [3, 6],\n",
    "            'clf__learning_rate': [0.01, 0.1],\n",
    "            'clf__subsample': [0.7, 1.0],\n",
    "            'clf__colsample_bytree': [0.7, 1.0],\n",
    "            'clf__min_child_samples': [10, 20],\n",
    "            'clf__reg_alpha': [0, 0.1],\n",
    "            'clf__reg_lambda': [1, 5],\n",
    "            'clf__class_weight': [None, 'balanced'],\n",
    "            'clf__boosting_type': ['gbdt']\n",
    "        },\n",
    "        \"cv\": 10\n",
    "    },\n",
    "    \"Support Vector Machine\": {\n",
    "        \"model\": SVC(probability=True, random_state=42),\n",
    "        \"params\": {\n",
    "            'clf__C': [0.01, 0.1, 1, 10],\n",
    "            'clf__kernel': ['linear', 'rbf'],\n",
    "            'clf__gamma': ['scale', 'auto']\n",
    "        },\n",
    "        \"cv\": 10\n",
    "    }\n",
    "}\n",
    "\n",
    "# =====================================================================\n",
    "# üìä Evaluation Helper\n",
    "# =====================================================================\n",
    "def evaluate_model(model, X, y):\n",
    "    \"\"\"Evaluate best estimator on training set with multiple metrics.\"\"\"\n",
    "    y_pred = model.predict(X)\n",
    "    y_prob = model.predict_proba(X)[:, 1]\n",
    "    tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
    "    return {\n",
    "        \"Accuracy (%)\": accuracy_score(y, y_pred) * 100,\n",
    "        \"Sensitivity\": recall_score(y, y_pred),\n",
    "        \"Specificity\": tn / (tn + fp),\n",
    "        \"Precision\": precision_score(y, y_pred),\n",
    "        \"AUC\": roc_auc_score(y, y_prob)\n",
    "    }\n",
    "\n",
    "# =====================================================================\n",
    "# üöÄ Main GridSearch Runner\n",
    "# =====================================================================\n",
    "def run_gridsearch(X, y):\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    results = {}\n",
    "\n",
    "    for name, cfg in models_grids.items():\n",
    "        print(f\"\\nüîç Tuning {name}...\")\n",
    "\n",
    "        # --- Pipeline: Scaling + SMOTE + Classifier ---\n",
    "        pipeline = ImbPipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('smote', SMOTE(random_state=42)),\n",
    "            ('clf', cfg['model'])\n",
    "        ])\n",
    "\n",
    "        # --- GridSearchCV ---\n",
    "        grid = GridSearchCV(\n",
    "            pipeline,\n",
    "            param_grid=cfg['params'],\n",
    "            scoring=scoring,\n",
    "            refit='roc_auc',\n",
    "            cv=StratifiedKFold(n_splits=cfg['cv'], shuffle=True, random_state=42),\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "        # Fit search\n",
    "        grid.fit(X, y)\n",
    "\n",
    "        # Evaluate best model\n",
    "        metrics = evaluate_model(grid.best_estimator_, X, y)\n",
    "\n",
    "        # Store results\n",
    "        results[name] = {\n",
    "            \"Best Params\": grid.best_params_,\n",
    "            **metrics\n",
    "        }\n",
    "\n",
    "        # Print quick summary\n",
    "        for k, v in metrics.items():\n",
    "            print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "    # --- Results DataFrame ---\n",
    "    results_df = pd.DataFrame(results).T.reset_index().rename(columns={'index': 'Model'})\n",
    "    print(\"\\nüìä Full Results:\")\n",
    "    print(results_df)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# üèÉ Run GridSearch on Training Data\n",
    "# =====================================================================\n",
    "label_col = 'Label'\n",
    "X = training_set.drop(columns=[label_col])\n",
    "y = training_set[label_col]\n",
    "\n",
    "results = run_gridsearch(X, y)\n",
    "\n",
    "# --- Save Results to CSV ---\n",
    "pd.DataFrame(results).T.reset_index().rename(columns={'index': 'Model'}).to_csv(OUTPUTS_DIR/ \"initial_gridsearch_results.csv\", index=False\n",
    ")\n",
    "print(\"\\n‚úÖ GridSearchCV results saved to 'OUTPUTS_DIR/initial_gridsearch_results.csv'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# üîé Feature Extraction & Ranking\n",
    "# =====================================================================\n",
    "\n",
    "# --- Standardize Features ---\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# =====================================================================\n",
    "# üìâ PCA Analysis\n",
    "# =====================================================================\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# --- Cumulative explained variance ---\n",
    "explained_var = np.cumsum(pca.explained_variance_ratio_)\n",
    "components = np.arange(1, len(explained_var) + 1)\n",
    "\n",
    "# --- Elbow detection with KneeLocator ---\n",
    "knee = KneeLocator(components, explained_var, curve='concave', direction='increasing')\n",
    "elbow_point = knee.knee\n",
    "elbow_value = explained_var[elbow_point - 1] if elbow_point else None\n",
    "\n",
    "# --- Plot PCA elbow ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(components, explained_var, marker='o', label=\"Cumulative Explained Variance\")\n",
    "plt.annotate(\n",
    "    f\"Elbow = {elbow_point}\",\n",
    "    xy=(elbow_point, elbow_value),\n",
    "    xytext=(elbow_point + 2, elbow_value - 0.1),\n",
    "    arrowprops=dict(facecolor='red', shrink=0.05),\n",
    "    fontsize=10,\n",
    "    color='red'\n",
    ")\n",
    "plt.xlabel(\"Number of Principal Components\")\n",
    "plt.ylabel(\"Cumulative Explained Variance\")\n",
    "plt.title(\"PCA Elbow Point with Arrow\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üîç PCA Elbow at {elbow_point} components ({elbow_value:.2%} variance explained)\")\n",
    "\n",
    "# =====================================================================\n",
    "# üìä Feature Importance Ranking\n",
    "# =====================================================================\n",
    "X = training_set.drop(columns=['Label'])\n",
    "y = training_set['Label']\n",
    "features = X.columns\n",
    "\n",
    "# --- Step 1: Mutual Information ---\n",
    "mi_scores = mutual_info_classif(X, y, random_state=42)\n",
    "\n",
    "# --- Step 2: Model-based Feature Importances ---\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X, y)\n",
    "rf_scores = rf.feature_importances_\n",
    "\n",
    "xgb = XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "xgb.fit(X, y)\n",
    "xgb_scores = xgb.feature_importances_\n",
    "\n",
    "lgbm = LGBMClassifier(n_estimators=100, random_state=42)\n",
    "lgbm.fit(X, y)\n",
    "lgbm_scores = lgbm.feature_importances_\n",
    "\n",
    "# --- Step 3: Combine Scores ---\n",
    "df = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Mutual_Info': mi_scores,\n",
    "    'RF_Importance': rf_scores,\n",
    "    'XGB_Importance': xgb_scores,\n",
    "    'LGBM_Importance': lgbm_scores\n",
    "})\n",
    "\n",
    "# --- Step 4: Compute Ranks ---\n",
    "for col in ['Mutual_Info', 'RF_Importance', 'XGB_Importance', 'LGBM_Importance']:\n",
    "    df[f'{col}_Rank'] = df[col].rank(ascending=False, method='min')\n",
    "\n",
    "# --- Step 5: Average rank across methods ---\n",
    "rank_cols = [col for col in df.columns if col.endswith('_Rank')]\n",
    "df['Avg_Rank'] = df[rank_cols].mean(axis=1)\n",
    "\n",
    "# --- Step 6: Sort & select Top 17 ---\n",
    "df_sorted = df.sort_values('Avg_Rank').reset_index(drop=True)\n",
    "top_17 = df_sorted.head(17)\n",
    "\n",
    "# --- Display ---\n",
    "print(\"\\nüèÜ Top 17 Features by Average Rank:\")\n",
    "print(top_17[['Feature', 'Mutual_Info', 'Mutual_Info_Rank',\n",
    "              'RF_Importance', 'RF_Importance_Rank',\n",
    "              'XGB_Importance', 'XGB_Importance_Rank',\n",
    "              'LGBM_Importance', 'LGBM_Importance_Rank',\n",
    "              'Avg_Rank']])\n",
    "\n",
    "# --- Save results to CSV ---\n",
    "top_17.to_csv(OUTPUTS_DIR/ \"top_17_features.csv\", index=False)\n",
    "print(\"\\n‚úÖ Top 17 features saved to 'outputs/top_17_features.csv'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# üß∞ Setup: Clean Column Names & Define Top-17 Features\n",
    "# =====================================================================\n",
    "\n",
    "# Clean column names\n",
    "training_set.columns = training_set.columns.str.strip().str.replace('\\xa0', ' ', regex=True)\n",
    "validation_set.columns = validation_set.columns.str.strip().str.replace('\\xa0', ' ', regex=True)\n",
    "\n",
    "# Handpicked Top-17 features\n",
    "top_17_features = [\n",
    "    'Creatine kinase isoenzymes', 'Albumin', 'Basophil ratio', 'Monocytes',\n",
    "    'Monocyte ratio', 'Creatine kinase', 'Lymphocyte ratio',\n",
    "    'Platelet distribution width', 'White blood cell', 'Alkaline phosphatase',\n",
    "    'Age', 'Aspartate aminotransferase', 'Lactate dehydrogenase',\n",
    "    'Glucose', 'Carbamide', 'Mean corpuscular volume', 'Globulin'\n",
    "]\n",
    "\n",
    "# Filter datasets\n",
    "training_set_filtered   = training_set[top_17_features]\n",
    "validation_set_filtered = validation_set[top_17_features]\n",
    "\n",
    "# Inputs\n",
    "label_col = 'Label'\n",
    "X = training_set_filtered\n",
    "y = training_set[label_col]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# üéØ Scorers (Accuracy, Sensitivity, Specificity, Precision, ROC AUC)\n",
    "# =====================================================================\n",
    "\n",
    "def specificity_score(y_true, y_pred):\n",
    "    \"\"\"Specificity = TN / (TN + FP).\"\"\"\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return tn / (tn + fp)\n",
    "\n",
    "def custom_auc(y_true, y_pred_proba):\n",
    "    \"\"\"AUC based on predicted probabilities.\"\"\"\n",
    "    return roc_auc_score(y_true, y_pred_proba)\n",
    "\n",
    "scoring = {\n",
    "    'accuracy':    make_scorer(accuracy_score),\n",
    "    'sensitivity': make_scorer(recall_score),\n",
    "    'specificity': make_scorer(specificity_score),\n",
    "    'precision':   make_scorer(precision_score),\n",
    "    'roc_auc':     make_scorer(custom_auc, needs_proba=True)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# ‚öôÔ∏è Model Grids (Top-17 Features)\n",
    "# =====================================================================\n",
    "\n",
    "models_grids = {\n",
    "    \"Logistic Regression\": {\n",
    "        \"model\": LogisticRegression(max_iter=10000, random_state=42),\n",
    "        \"params\": {\n",
    "            'clf__C': [0.01, 0.1, 1, 10],\n",
    "            'clf__penalty': ['l2'],\n",
    "            'clf__solver': ['liblinear', 'lbfgs'],\n",
    "            'clf__class_weight': [None, 'balanced'],\n",
    "            'clf__fit_intercept': [True, False]\n",
    "        },\n",
    "        \"cv\": 5\n",
    "    },\n",
    "    \"Naive Bayes\": {\n",
    "        \"model\": GaussianNB(),\n",
    "        \"params\": {\n",
    "            'clf__var_smoothing': [1e-9, 1e-8, 1e-7]\n",
    "        },\n",
    "        \"cv\": 5\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        \"model\": RandomForestClassifier(random_state=42),\n",
    "        \"params\": {\n",
    "            'clf__n_estimators': [100, 200],\n",
    "            'clf__max_depth': [5, 10, 15],\n",
    "            'clf__min_samples_split': [2, 5],\n",
    "            'clf__min_samples_leaf': [2, 4],\n",
    "            'clf__max_features': ['sqrt', 'log2'],\n",
    "            'clf__class_weight': ['balanced'],\n",
    "            'clf__criterion': ['gini', 'entropy']\n",
    "        },\n",
    "        \"cv\": 5\n",
    "    },\n",
    "    \"AdaBoost\": {\n",
    "        \"model\": AdaBoostClassifier(random_state=42),\n",
    "        \"params\": {\n",
    "            'clf__n_estimators': [50, 100, 200],\n",
    "            'clf__learning_rate': [0.01, 0.1, 0.5, 1.0],\n",
    "            'clf__algorithm': ['SAMME', 'SAMME.R'],\n",
    "            'clf__estimator': [DecisionTreeClassifier(max_depth=d) for d in [1, 2, 3]]\n",
    "        },\n",
    "        \"cv\": 5\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"model\": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),\n",
    "        \"params\": {\n",
    "            'clf__n_estimators': [100, 200],\n",
    "            'clf__max_depth': [3, 6],\n",
    "            'clf__learning_rate': [0.01, 0.1],\n",
    "            'clf__subsample': [0.7, 1.0],\n",
    "            'clf__colsample_bytree': [0.7, 1.0],\n",
    "            'clf__gamma': [0, 1],\n",
    "            'clf__min_child_weight': [1, 3],\n",
    "            'clf__reg_alpha': [0, 0.1],\n",
    "            'clf__reg_lambda': [1, 5],\n",
    "            'clf__scale_pos_weight': [1, 2]\n",
    "        },\n",
    "        \"cv\": 7\n",
    "    },\n",
    "    \"LightGBM\": {\n",
    "        \"model\": LGBMClassifier(random_state=42),\n",
    "        \"params\": {\n",
    "            'clf__n_estimators': [100, 200],\n",
    "            'clf__max_depth': [3, 6],\n",
    "            'clf__learning_rate': [0.01, 0.1],\n",
    "            'clf__subsample': [0.7, 1.0],\n",
    "            'clf__colsample_bytree': [0.7, 1.0],\n",
    "            'clf__min_child_samples': [10, 20],\n",
    "            'clf__reg_alpha': [0, 0.1],\n",
    "            'clf__reg_lambda': [1, 5],\n",
    "            'clf__class_weight': [None, 'balanced'],\n",
    "            'clf__boosting_type': ['gbdt']\n",
    "        },\n",
    "        \"cv\": 10\n",
    "    },\n",
    "    \"Support Vector Machine\": {\n",
    "        \"model\": SVC(probability=True, random_state=42),\n",
    "        \"params\": {\n",
    "            'clf__C': [0.01, 0.1, 1, 10],\n",
    "            'clf__kernel': ['linear', 'rbf'],\n",
    "            'clf__gamma': ['scale', 'auto']\n",
    "        },\n",
    "        \"cv\": 10\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# üìä Evaluation Helper\n",
    "# =====================================================================\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    \"\"\"Return a dict of metrics for a fitted model on given X/y.\"\"\"\n",
    "    y_pred = model.predict(X)\n",
    "    y_prob = model.predict_proba(X)[:, 1]\n",
    "    tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
    "    return {\n",
    "        \"Accuracy (%)\": accuracy_score(y, y_pred) * 100,\n",
    "        \"Sensitivity\":   recall_score(y, y_pred),\n",
    "        \"Specificity\":   tn / (tn + fp),\n",
    "        \"Precision\":     precision_score(y, y_pred),\n",
    "        \"AUC\":           roc_auc_score(y, y_prob)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# üöÄ GridSearch Runner (Top-17 features)\n",
    "# =====================================================================\n",
    "\n",
    "def run_gridsearch(X, y):\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    results = {}\n",
    "\n",
    "    for name, cfg in models_grids.items():\n",
    "        print(f\"\\nüîç Tuning {name}...\")\n",
    "\n",
    "        # Pipeline: scale ‚Üí SMOTE ‚Üí classifier\n",
    "        pipeline = ImbPipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('smote',  SMOTE(random_state=42)),\n",
    "            ('clf',    cfg['model'])\n",
    "        ])\n",
    "\n",
    "        grid = GridSearchCV(\n",
    "            pipeline,\n",
    "            param_grid=cfg['params'],\n",
    "            scoring=scoring,\n",
    "            refit='roc_auc',\n",
    "            cv=StratifiedKFold(n_splits=cfg['cv'], shuffle=True, random_state=42),\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "        grid.fit(X, y)\n",
    "        metrics = evaluate_model(grid.best_estimator_, X, y)\n",
    "\n",
    "        results[name] = {\"Best Params\": grid.best_params_, **metrics}\n",
    "\n",
    "        # Quick on-screen summary\n",
    "        for k, v in metrics.items():\n",
    "            print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "    results_df = pd.DataFrame(results).T.reset_index().rename(columns={'index': 'Model'})\n",
    "    print(\"\\nüìä Full Results:\")\n",
    "    print(results_df)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# üèÅ Run Search + Save Results (Top-17)\n",
    "# =====================================================================\n",
    "\n",
    "results = run_gridsearch(X, y)\n",
    "\n",
    "# Save to CSV (metrics + best params per model)\n",
    "import os, json\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "results_df = pd.DataFrame(results).T.reset_index().rename(columns={'index': 'Model'})\n",
    "\n",
    "# Expand \"Best Params\" dict into columns for easy reading\n",
    "best_params_df = pd.json_normalize(results_df[\"Best Params\"])\n",
    "final_df = pd.concat([results_df.drop(columns=[\"Best Params\"]), best_params_df], axis=1)\n",
    "\n",
    "final_df.to_csv(OUTPUTS_DIR/ \"gridsearch_results_top17.csv\", index=False)\n",
    "print(\"üíæ Saved: outputs/gridsearch_results_top17.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# üß™ Validation Set Evaluation (Top-17 Features)\n",
    "# =====================================================================\n",
    "\n",
    "# --- Input datasets ---\n",
    "X_train = training_set_filtered\n",
    "y_train = training_set['Label']\n",
    "X_val   = validation_set_filtered\n",
    "y_val   = validation_set_label\n",
    "\n",
    "# =====================================================================\n",
    "# üéØ Custom Metrics\n",
    "# =====================================================================\n",
    "\n",
    "def specificity_score(y_true, y_pred):\n",
    "    \"\"\"Specificity = TN / (TN + FP).\"\"\"\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return tn / (tn + fp)\n",
    "\n",
    "def get_metrics(model, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Fit model and return metrics on validation set.\"\"\"\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    # Probabilities (handle classifiers without predict_proba)\n",
    "    try:\n",
    "        y_prob = model.predict_proba(X_val)[:, 1]\n",
    "    except:\n",
    "        y_prob = model.decision_function(X_val)\n",
    "        # Normalize to [0,1]\n",
    "        y_prob = (y_prob - y_prob.min()) / (y_prob.max() - y_prob.min())\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_val, y_pred).ravel()\n",
    "\n",
    "    return {\n",
    "        \"Accuracy (%)\": accuracy_score(y_val, y_pred) * 100,\n",
    "        \"Sensitivity\":  recall_score(y_val, y_pred),\n",
    "        \"Specificity\":  specificity_score(y_val, y_pred),\n",
    "        \"Precision\":    precision_score(y_val, y_pred),\n",
    "        \"AUC\":          roc_auc_score(y_val, y_prob)\n",
    "    }\n",
    "\n",
    "# =====================================================================\n",
    "# ‚öôÔ∏è Final Models (with Tuned Hyperparameters)\n",
    "# =====================================================================\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(\n",
    "        C=0.01, penalty='l2', solver='liblinear',\n",
    "        class_weight=None, fit_intercept=True,\n",
    "        max_iter=1000, random_state=42\n",
    "    ),\n",
    "\n",
    "    \"Naive Bayes\": GaussianNB(var_smoothing=1e-9),\n",
    "\n",
    "    \"Random Forest\": RandomForestClassifier(\n",
    "        n_estimators=100, max_depth=5, min_samples_split=2,\n",
    "        min_samples_leaf=2, max_features='sqrt', criterion='gini',\n",
    "        class_weight='balanced', random_state=42\n",
    "    ),\n",
    "\n",
    "    \"AdaBoost\": AdaBoostClassifier(\n",
    "        estimator=DecisionTreeClassifier(max_depth=1),\n",
    "        n_estimators=50, learning_rate=0.01,\n",
    "        algorithm='SAMME', random_state=42\n",
    "    ),\n",
    "\n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        n_estimators=100, max_depth=3, learning_rate=0.01,\n",
    "        subsample=0.7, colsample_bytree=0.7, gamma=0,\n",
    "        min_child_weight=1, reg_alpha=0, reg_lambda=1,\n",
    "        scale_pos_weight=1, use_label_encoder=False,\n",
    "        eval_metric='logloss', random_state=42\n",
    "    ),\n",
    "\n",
    "    \"LightGBM\": LGBMClassifier(\n",
    "        n_estimators=100, max_depth=3, learning_rate=0.01,\n",
    "        subsample=0.7, colsample_bytree=0.7, min_child_samples=10,\n",
    "        reg_alpha=0, reg_lambda=1, class_weight=None,\n",
    "        boosting_type='gbdt', random_state=42\n",
    "    ),\n",
    "\n",
    "    \"Support Vector Machine\": SVC(\n",
    "        probability=True, kernel='linear', gamma='scale',\n",
    "        C=0.01, random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "# =====================================================================\n",
    "# üöÄ Run Evaluation\n",
    "# =====================================================================\n",
    "\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    metrics = get_metrics(model, X_train, y_train, X_val, y_val)\n",
    "    metrics['Model'] = name\n",
    "    results.append(metrics)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# --- Display in notebook ---\n",
    "print(\"\\nüìä Final Model Performance on Validation Set (Top 17 Features)\")\n",
    "print(results_df.round(3).set_index(\"Model\"))\n",
    "\n",
    "# =====================================================================\n",
    "# üíæ Save Results to CSV\n",
    "# =====================================================================\n",
    "import os\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "results_df.round(4).to_csv(OUTPUTS_DIR/ \"validation_results_top17.csv\", index=False)\n",
    "print(\"üíæ Saved: outputs/validation_results_top17.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# üéØ THRESHOLD OPTIMIZATION\n",
    "# =====================================================================\n",
    "\n",
    "# --- Custom Specificity ---\n",
    "def specificity_score(y_true, y_pred):\n",
    "    \"\"\"Specificity = TN / (TN + FP).\"\"\"\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return tn / (tn + fp)\n",
    "\n",
    "# --- Evaluate metrics across thresholds for a given model ---\n",
    "def evaluate_thresholds(model, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Fit model on train, score on val across thresholds.\"\"\"\n",
    "    model.fit(X_train, y_train)\n",
    "    probas = model.predict_proba(X_val)[:, 1]\n",
    "    thresholds = np.arange(0.1, 0.91, 0.1)\n",
    "    rows = []\n",
    "\n",
    "    for t in thresholds:\n",
    "        preds = (probas >= t).astype(int)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_val, preds).ravel()\n",
    "\n",
    "        rows.append({\n",
    "            \"Threshold\": t,\n",
    "            \"Accuracy (%)\": accuracy_score(y_val, preds) * 100,\n",
    "            \"Sensitivity\":  recall_score(y_val, preds),\n",
    "            \"Specificity\":  specificity_score(y_val, preds),\n",
    "            \"Precision\":    precision_score(y_val, preds),\n",
    "            \"AUC\":          roc_auc_score(y_val, probas)  # AUC constant across thresholds\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# =====================================================================\n",
    "# üß∞ Inputs (Top-17 Features)\n",
    "# =====================================================================\n",
    "X_train = training_set_filtered\n",
    "y_train = training_set['Label']\n",
    "X_val   = validation_set_filtered\n",
    "y_val   = validation_set_label\n",
    "\n",
    "# =====================================================================\n",
    "# ü§ñ Models (final tuned hyperparameters)\n",
    "# =====================================================================\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(\n",
    "        n_estimators=100, max_depth=5, min_samples_split=2, min_samples_leaf=2,\n",
    "        max_features='sqrt', criterion='gini', class_weight='balanced', random_state=42\n",
    "    ),\n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        n_estimators=100, max_depth=3, learning_rate=0.01, subsample=0.7, colsample_bytree=0.7,\n",
    "        gamma=0, min_child_weight=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
    "        use_label_encoder=False, eval_metric='logloss', random_state=42\n",
    "    ),\n",
    "    \"LightGBM\": LGBMClassifier(\n",
    "        n_estimators=100, max_depth=3, learning_rate=0.01, subsample=0.7, colsample_bytree=0.7,\n",
    "        min_child_samples=10, reg_alpha=0, reg_lambda=1, class_weight=None,\n",
    "        boosting_type='gbdt', random_state=42\n",
    "    ),\n",
    "    \"Logistic Regression\": LogisticRegression(\n",
    "        C=0.01, penalty='l2', solver='liblinear', class_weight=None,\n",
    "        fit_intercept=True, max_iter=1000, random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "# =====================================================================\n",
    "# üöÄ Run threshold sweep for each model\n",
    "# =====================================================================\n",
    "threshold_results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nüîç Threshold tuning: {name}\")\n",
    "    df = evaluate_thresholds(model, X_train, y_train, X_val, y_val)\n",
    "    threshold_results[name] = df\n",
    "    print(df.round(3))\n",
    "\n",
    "# =====================================================================\n",
    "# üíæ Export: full grid + best threshold per model\n",
    "# =====================================================================\n",
    "import os\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "# Full combined table (all models √ó thresholds)\n",
    "combined_df = pd.concat({k: v for k, v in threshold_results.items()}, names=[\"Model\"])\n",
    "combined_df = combined_df.reset_index()  # columns: Model, level_1 (drop), Threshold, ...\n",
    "combined_df = combined_df.drop(columns=[\"level_1\"], errors=\"ignore\")\n",
    "combined_df.to_csv(OUTPUTS_DIR/\"threshold_optimization_results.csv\", index=False)\n",
    "\n",
    "# Best threshold per model by Youden‚Äôs J (Sensitivity + Specificity ‚àí 1)\n",
    "best_rows = []\n",
    "for model_name, df in threshold_results.items():\n",
    "    df = df.copy()\n",
    "    df[\"Youden_J\"] = df[\"Sensitivity\"] + df[\"Specificity\"] - 1.0\n",
    "    best = df.sort_values(\"Youden_J\", ascending=False).iloc[0]\n",
    "    best_rows.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Best Threshold\": best[\"Threshold\"],\n",
    "        \"Youden_J\": best[\"Youden_J\"],\n",
    "        \"Accuracy (%)\": best[\"Accuracy (%)\"],\n",
    "        \"Sensitivity\": best[\"Sensitivity\"],\n",
    "        \"Specificity\": best[\"Specificity\"],\n",
    "        \"Precision\": best[\"Precision\"],\n",
    "        \"AUC\": best[\"AUC\"]\n",
    "    })\n",
    "\n",
    "best_df = pd.DataFrame(best_rows).sort_values(by=[\"AUC\",\"Youden_J\"], ascending=False)\n",
    "best_df.to_csv(OUTPUTS_DIR/ \"threshold_best_by_model.csv\", index=False)\n",
    "\n",
    "print(\"\\n‚úÖ Threshold optimization complete.\")\n",
    "print(\"üíæ Saved: outputs/threshold_optimization_results.csv\")\n",
    "print(\"üíæ Saved: outputs/threshold_best_by_model.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# üíæ Save XGB-LC after threshold selection (refit on training, then save)\n",
    "# =====================================================================\n",
    "\n",
    "# 1) Point to  training split used for validation\n",
    "X_train = training_set_filtered            # top-17 features\n",
    "y_train = training_set['Label']\n",
    "\n",
    "# 2) Refit the chosen model (same hyperparams that were validated)\n",
    "best_model = XGBClassifier(\n",
    "    n_estimators=100, max_depth=3, learning_rate=0.01,\n",
    "    subsample=0.7, colsample_bytree=0.7, gamma=0,\n",
    "    min_child_weight=1, reg_alpha=0, reg_lambda=1,\n",
    "    scale_pos_weight=1, use_label_encoder=False,\n",
    "    eval_metric='logloss', random_state=42\n",
    ")\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# 3) Save artifacts (model + metadata)\n",
    "\n",
    "MODEL_PATH = os.path.join(MODELS_DIR, \"XGB-LC.pkl\")\n",
    "META_PATH  = os.path.join(MODELS_DIR, \"XGB-LC_meta.json\")\n",
    "\n",
    "joblib.dump(best_model, MODEL_PATH)\n",
    "\n",
    "meta = {\n",
    "    \"model_name\": \"XGB-LC\",\n",
    "    \"created_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"threshold\": 0.70,                           # <- your chosen threshold\n",
    "    \"feature_names\": list(X_train.columns),      # exact training feature order\n",
    "    \"label_col\": \"Label\",\n",
    "    \"notes\": \"Selected on validation; do NOT refit before test. Use fixed threshold.\"\n",
    "}\n",
    "with open(META_PATH, \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Saved model to {MODEL_PATH}\")\n",
    "print(f\"‚úÖ Saved metadata to {META_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# üîÅ Load XGB-LC & Evaluate on Test Set\n",
    "# =====================================================================\n",
    "\n",
    "# --- Paths to saved artifacts (from your Save step) ---\n",
    "MODEL_PATH = MODELS_DIR/ \"XGB-LC.pkl\"\n",
    "META_PATH  = MODELS_DIR/ \"XGB-LC_meta.json\"\n",
    "\n",
    "# --- Load model & metadata ---\n",
    "model = joblib.load(MODEL_PATH)\n",
    "with open(META_PATH, \"r\") as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "threshold     = float(meta[\"threshold\"])\n",
    "feature_names = meta[\"features\"] if \"features\" in meta else meta.get(\"feature_names\", [])\n",
    "label_col     = meta.get(\"label_col\", \"Label\")\n",
    "\n",
    "# --- Prepare test data ---\n",
    "# Ensure label is normalized and align columns to the training feature order\n",
    "test_set = test_set.copy()\n",
    "test_set[label_col] = test_set[label_col].replace({-1: 0, 1: 1})\n",
    "\n",
    "X_test_raw = test_set.drop(columns=[label_col])\n",
    "y_test     = test_set[label_col]\n",
    "\n",
    "# Align test columns to training columns (adds any missing as NaN, drops extras)\n",
    "X_test = X_test_raw.reindex(columns=feature_names)\n",
    "\n",
    "# --- Predict ---\n",
    "probs = model.predict_proba(X_test)[:, 1]\n",
    "preds = (probs >= threshold).astype(int)\n",
    "\n",
    "# --- Metrics ---\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, preds).ravel()\n",
    "acc  = accuracy_score(y_test, preds) * 100\n",
    "sens = recall_score(y_test, preds)\n",
    "spec = tn / (tn + fp) if (tn + fp) else np.nan\n",
    "prec = precision_score(y_test, preds)\n",
    "auc  = roc_auc_score(y_test, probs)\n",
    "\n",
    "print(\"\\nüß™ External Test Set ‚Äî XGB-LC @ fixed threshold\")\n",
    "print(f\"Threshold:    {threshold:.2f}\")\n",
    "print(f\"Accuracy (%): {acc:.3f}\")\n",
    "print(f\"Sensitivity:  {sens:.3f}\")\n",
    "print(f\"Specificity:  {spec:.3f}\")\n",
    "print(f\"Precision:    {prec:.3f}\")\n",
    "print(f\"AUC:          {auc:.3f}\")\n",
    "\n",
    "# --- Save output ---\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "metrics = {\n",
    "    \"model_name\": meta.get(\"model_name\", \"XGB-LC\"),\n",
    "    \"threshold\": threshold,\n",
    "    \"features_used\": feature_names,\n",
    "    \"Accuracy_%\": round(acc, 4),\n",
    "    \"Sensitivity\": round(sens, 4),\n",
    "    \"Specificity\": round(spec, 4),\n",
    "    \"Precision\": round(prec, 4),\n",
    "    \"AUC\": round(auc, 4),\n",
    "}\n",
    "\n",
    "pd.DataFrame([metrics]).to_csv(OUTPUTS_DIR/ \"test_metrics_XGB-LC.csv\", index=False)\n",
    "print(\"üíæ Saved: outputs/test_metrics_XGB-LC.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
